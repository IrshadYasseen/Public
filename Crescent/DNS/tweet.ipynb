{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7134997,"sourceType":"datasetVersion","datasetId":4116921}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-05T11:43:15.993173Z","iopub.execute_input":"2024-01-05T11:43:15.99364Z","iopub.status.idle":"2024-01-05T11:43:16.003225Z","shell.execute_reply.started":"2024-01-05T11:43:15.993583Z","shell.execute_reply":"2024-01-05T11:43:16.001862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/tweets-and-user-engagement/Twitterdatainsheets.csv')","metadata":{"execution":{"iopub.status.busy":"2024-01-08T11:59:53.890439Z","iopub.execute_input":"2024-01-08T11:59:53.890865Z","iopub.status.idle":"2024-01-08T11:59:54.814018Z","shell.execute_reply.started":"2024-01-08T11:59:53.89083Z","shell.execute_reply":"2024-01-08T11:59:54.812906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info","metadata":{"execution":{"iopub.status.busy":"2024-01-08T11:59:58.834684Z","iopub.execute_input":"2024-01-08T11:59:58.835122Z","iopub.status.idle":"2024-01-08T11:59:58.869014Z","shell.execute_reply.started":"2024-01-08T11:59:58.835087Z","shell.execute_reply":"2024-01-08T11:59:58.867726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.columns","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:43:17.381133Z","iopub.execute_input":"2024-01-05T11:43:17.381573Z","iopub.status.idle":"2024-01-05T11:43:17.390275Z","shell.execute_reply.started":"2024-01-05T11:43:17.381536Z","shell.execute_reply":"2024-01-05T11:43:17.389038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean up column names\ndf.columns = df.columns.str.strip()\n\n# Check the cleaned column names\ndf.columns\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:43:17.393271Z","iopub.execute_input":"2024-01-05T11:43:17.393648Z","iopub.status.idle":"2024-01-05T11:43:17.40122Z","shell.execute_reply.started":"2024-01-05T11:43:17.393616Z","shell.execute_reply":"2024-01-05T11:43:17.400386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# Check for missing values\nmissing_values = df.isnull().sum()\n\n# Display data types\ndata_types = df.dtypes\n\n# Display the results\nprint(\"Missing Values:\")\nprint(missing_values)\n\nprint(\"\\nData Types:\")\nprint(data_types)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:43:17.402899Z","iopub.execute_input":"2024-01-05T11:43:17.403273Z","iopub.status.idle":"2024-01-05T11:43:17.553914Z","shell.execute_reply.started":"2024-01-05T11:43:17.40324Z","shell.execute_reply":"2024-01-05T11:43:17.552785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code cell is performing an initial check on the dataset (df) to identify and analyze missing values. It also provides information about the data types of each column","metadata":{}},{"cell_type":"code","source":"# Drop rows with missing values for specific columns\ndf_cleaned = df.dropna(subset=['TweetID', 'Weekday', 'Hour', 'Day', 'Lang'])\n\n# Display the shape of the cleaned DataFrame\nprint(\"Shape after handling missing values:\", df_cleaned.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:43:17.55542Z","iopub.execute_input":"2024-01-05T11:43:17.55585Z","iopub.status.idle":"2024-01-05T11:43:17.69196Z","shell.execute_reply.started":"2024-01-05T11:43:17.555801Z","shell.execute_reply":"2024-01-05T11:43:17.69044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code cell is aimed at cleaning the dataset (df) by removing rows that have missing values in specific columns.","metadata":{}},{"cell_type":"markdown","source":"**Drop Rows with Missing Values:**\n\nThe dropna() method is used to remove rows with missing values from the specified columns ('TweetID', 'Weekday', 'Hour', 'Day', and 'Lang').\nThe cleaned DataFrame is stored in the variable df_cleaned.\nDisplay the Shape of the Cleaned DataFrame:\n\nThe shape attribute is used to obtain the dimensions (number of rows and columns) of the cleaned DataFrame.\nThe results are printed to the console.","metadata":{}},{"cell_type":"code","source":"# Define numeric_columns based on the numeric columns in your DataFrame\nnumeric_columns = ['Hour', 'Day', 'IsReshare', 'Reach', 'RetweetCount', 'Likes', 'Klout', 'Sentiment', 'LocationID']\n\n# Convert numeric columns to appropriate data types using .loc\ndf_cleaned.loc[:, numeric_columns] = df_cleaned[numeric_columns].apply(pd.to_numeric, errors='coerce')\n\n# Display the data types after conversion\nprint(\"\\nData Types after conversion:\")\nprint(df_cleaned.dtypes)\n\n# Check for missing values again\nmissing_values_cleaned = df_cleaned.isnull().sum()\nprint(\"\\nMissing Values after cleaning:\")\nprint(missing_values_cleaned)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:43:17.693247Z","iopub.execute_input":"2024-01-05T11:43:17.693605Z","iopub.status.idle":"2024-01-05T11:43:18.018409Z","shell.execute_reply.started":"2024-01-05T11:43:17.693576Z","shell.execute_reply":"2024-01-05T11:43:18.017198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code cell is responsible for converting the numeric columns in the cleaned DataFrame (df_cleaned) to appropriate data types using the .loc accessor.","metadata":{}},{"cell_type":"code","source":"# Impute missing values for numeric columns using .loc\ndf_cleaned.loc[:, numeric_columns] = df_cleaned.loc[:, numeric_columns].apply(lambda x: x.fillna(x.mean()))\n\n# Display the DataFrame after imputation\nprint(df_cleaned.head())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:43:18.020032Z","iopub.execute_input":"2024-01-05T11:43:18.020499Z","iopub.status.idle":"2024-01-05T11:43:18.151922Z","shell.execute_reply.started":"2024-01-05T11:43:18.020462Z","shell.execute_reply":"2024-01-05T11:43:18.150786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code cell is focused on imputing missing values in the numeric columns of the cleaned DataFrame (df_cleaned) using the mean value of each column.","metadata":{}},{"cell_type":"markdown","source":"## Data Visualization","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Basic statistics for numeric columns\nnumeric_stats = df_cleaned[numeric_columns].describe()\nprint(\"Basic Statistics for Numeric Columns:\")\nprint(numeric_stats)\n\n# Subsample the data (adjust n as needed)\ndf_subsample = df_cleaned.sample(n=5000)\n\n# Correlation heatmap for numeric columns\nplt.figure(figsize=(12, 8))\nsns.heatmap(df_subsample[numeric_columns].corr(), annot=True, cmap='coolwarm', linewidths=.5)\nplt.title('Correlation Heatmap for Numeric Columns (Subsample)')\nplt.show()\n\n# Pairplot for numeric columns\nsns.pairplot(df_subsample[numeric_columns])\nplt.suptitle('Pairplot for Numeric Columns (Subsample)', y=1.02)\nplt.show()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:45:44.189836Z","iopub.execute_input":"2024-01-05T11:45:44.190243Z","iopub.status.idle":"2024-01-05T11:46:23.88411Z","shell.execute_reply.started":"2024-01-05T11:45:44.190211Z","shell.execute_reply":"2024-01-05T11:46:23.882779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code cell is responsible for generating visualizations and basic statistics for numeric columns in the cleaned DataFrame (df_cleaned). ","metadata":{}},{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"## Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"pip install nltk\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:44:01.385505Z","iopub.execute_input":"2024-01-05T11:44:01.386439Z","iopub.status.idle":"2024-01-05T11:44:36.623219Z","shell.execute_reply.started":"2024-01-05T11:44:01.386394Z","shell.execute_reply":"2024-01-05T11:44:36.621951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# Download the VADER lexicon for sentiment analysis\nnltk.download('vader_lexicon')\n\n# Initialize the SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\n\n# Analyze sentiment for each tweet\ndf_cleaned['SentimentScore'] = df_cleaned['text'].dropna().apply(lambda x: sia.polarity_scores(x)['compound'])\n\n# Visualize sentiment distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(df_cleaned['SentimentScore'], bins=30, kde=True)\nplt.title('Distribution of Sentiment Scores in Tweets')\nplt.xlabel('Sentiment Score')\nplt.ylabel('Frequency')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:44:36.624691Z","iopub.execute_input":"2024-01-05T11:44:36.625054Z","iopub.status.idle":"2024-01-05T11:45:27.695239Z","shell.execute_reply.started":"2024-01-05T11:44:36.625008Z","shell.execute_reply":"2024-01-05T11:45:27.693856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code cell focuses on sentiment analysis of the tweets using the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool. Let's go through each part of the code:\n\nDownload VADER Lexicon:\n\nThe nltk.download('vader_lexicon') line downloads the VADER lexicon, a pre-built sentiment analysis lexicon used by the NLTK library.\nInitialize SentimentIntensityAnalyzer:\n\nThe SentimentIntensityAnalyzer from the NLTK's VADER module is initialized. This analyzer provides a compound sentiment score for a given text.\nSentiment Analysis:\n\nThe sentiment of each tweet is analyzed using the VADER sentiment analyzer.\nThe compound sentiment score is obtained for each tweet's text by applying sia.polarity_scores(x)['compound'].\nVisualize Sentiment Distribution:\n\nA histogram is created using Seaborn (sns.histplot) to visualize the distribution of sentiment scores in the tweets.\nThe x-axis represents the sentiment scores, and the y-axis represents the frequency of tweets with a specific sentiment score.\nThe title, xlabel, and ylabel are set for better interpretation of the plot.","metadata":{}},{"cell_type":"markdown","source":"## Test Model","metadata":{}},{"cell_type":"markdown","source":"This code cell is designed to analyze the sentiment of a specific tweet in the dataset. Here's a breakdown of each part:\n\nChoose a Specific Tweet:\n\nThe variable specific_tweet_index is set to a specific index value. You can change this value to the index of the tweet you want to analyze.\nRetrieve Text of the Specific Tweet:\n\nUsing df_cleaned.loc[specific_tweet_index, 'text'], the text content of the specified tweet is extracted from the DataFrame.\nCalculate Sentiment Score:\n\nThe VADER SentimentIntensityAnalyzer (sia) is employed to compute the sentiment score for the specific tweet.\nThe compound sentiment score is obtained by applying sia.polarity_scores(specific_tweet_text)['compound'].\nPrint Results:\n\nThe text content of the specific tweet is printed for reference.\nThe calculated sentiment score for the specific tweet is printed as well.","metadata":{}},{"cell_type":"code","source":"# Choose a specific tweet index\nspecific_tweet_index = 302  # Change this index to the tweet you want to analyze\n\n# Get the text of the specific tweet\nspecific_tweet_text = df_cleaned.loc[specific_tweet_index, 'text']\n\n# Calculate the sentiment score for the specific tweet\nspecific_tweet_sentiment_score = sia.polarity_scores(specific_tweet_text)['compound']\n\n# Print the results\nprint(f\"Text of the specific tweet:\\n{specific_tweet_text}\\n\")\nprint(f\"Sentiment Score for the specific tweet: {specific_tweet_sentiment_score}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:45:27.697304Z","iopub.execute_input":"2024-01-05T11:45:27.698222Z","iopub.status.idle":"2024-01-05T11:45:27.708526Z","shell.execute_reply.started":"2024-01-05T11:45:27.698074Z","shell.execute_reply":"2024-01-05T11:45:27.70732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose a specific tweet index\nspecific_tweet_index = 32  # Change this index to the tweet you want to analyze\n\n# Get the text of the specific tweet\nspecific_tweet_text = df_cleaned.loc[specific_tweet_index, 'text']\n\n# Calculate the sentiment score for the specific tweet\nspecific_tweet_sentiment_score = sia.polarity_scores(specific_tweet_text)['compound']\n\n# Convert sentiment score to a categorical label\nif specific_tweet_sentiment_score > 0:\n    sentiment_label = 'positive'\nelif specific_tweet_sentiment_score < 0:\n    sentiment_label = 'negative'\nelse:\n    sentiment_label = 'neutral'\n\n# Print the results\nprint(f\"Text of the specific tweet:\\n{specific_tweet_text}\\n\")\nprint(f\"Sentiment Score for the specific tweet: {specific_tweet_sentiment_score}\")\nprint(f\"Sentiment Label for the specific tweet: {sentiment_label}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:45:27.712601Z","iopub.execute_input":"2024-01-05T11:45:27.713643Z","iopub.status.idle":"2024-01-05T11:45:27.722137Z","shell.execute_reply.started":"2024-01-05T11:45:27.713603Z","shell.execute_reply":"2024-01-05T11:45:27.720748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Choose a specific tweet index\nspecific_tweet_index = 22588  # Change this index to the tweet you want to analyze\n\n# Get the text of the specific tweet\nspecific_tweet_text = df_cleaned.loc[specific_tweet_index, 'text']\n\n# Calculate the sentiment score for the specific tweet\nspecific_tweet_sentiment_score = sia.polarity_scores(specific_tweet_text)['compound']\n\n# Convert sentiment score to a categorical label\nif specific_tweet_sentiment_score > 0:\n    sentiment_label = 'positive'\nelif specific_tweet_sentiment_score < 0:\n    sentiment_label = 'negative'\nelse:\n    sentiment_label = 'neutral'\n\n# Print the results\nprint(f\"Text of the specific tweet:\\n{specific_tweet_text}\\n\")\nprint(f\"Sentiment Score for the specific tweet: {specific_tweet_sentiment_score}\")\nprint(f\"Sentiment Label for the specific tweet: {sentiment_label}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T11:45:27.724026Z","iopub.execute_input":"2024-01-05T11:45:27.724834Z","iopub.status.idle":"2024-01-05T11:45:27.736665Z","shell.execute_reply.started":"2024-01-05T11:45:27.724787Z","shell.execute_reply":"2024-01-05T11:45:27.735184Z"},"trusted":true},"execution_count":null,"outputs":[]}]}